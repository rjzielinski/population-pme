%\documentclass[12pt]{amsart}
\documentclass[11pt,reqno]{article}
%\usepackage{cases}

%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
\RequirePackage{graphicx}%% uncomment this for including figures

\usepackage[
  backend=biber,
  style=authoryear
]{biblatex}

\addbibresource{references.bib}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage[perpage,symbol*]{footmisc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}  
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm2e}

%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={(},close={)}}
\usepackage{authblk}

\usepackage{csquotes}
\usepackage[english]{babel}

\renewcommand{\baselinestretch}{1.0}
\setlength{\oddsidemargin}{-0.5cm}
\setlength{\evensidemargin}{-0.5cm}
\renewcommand{\topmargin}{-2cm}
\renewcommand{\oddsidemargin}{0mm}
\renewcommand{\evensidemargin}{0mm}
\renewcommand{\textwidth}{180mm}
\renewcommand{\textheight}{240mm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\T}{\intercal}
\newcommand{\commentout}[1]{}

\newcommand{\kmedit}[1]{{\color{purple}  #1}}
\newcommand{\meng}[1]{{\color{purple} \sf $\clubsuit\clubsuit\clubsuit$ Kun Meng: [#1]}}
\newcommand{\Meng}[1]{\margMa{(Kun Meng) #1}}

\newcommand{\zielinski}[1]{{\color{blue} \sf $\spadesuit\spadesuit\spadesuit$ Rob Zielinski: [#1]}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\title{Hierarchical Principal Manifold Estimation}
\author[1]{Robert Zielinski}
\author[2]{Kun Meng}
\author[1]{Ani Eloyan}
\affil[1]{Department of Biostatistics, Brown University}
\affil[2]{Division of Applied Mathematics, Brown University}



\maketitle

\doublespacing

\section*{Abstract}

\section{Introduction}

Neurodegenerative diseases such as Alzheimer's disease (AD) and Parkinson's disease are highly complex, chronic conditions that exhibit substantial heterogeneity between patients. To enable researchers to identify potential biomarkers of progression for these conditions, large multi-site longitudinal observational studies have been conducted that collect information from a wide range of participants over long durations across several modalities, including neuroimaging [Cite ADNI and PPMI studies]. Neuroimaging data in particular have been shown to be sensitive to developments in the early stages of disease progression, often before diagnosis occurs [Find citation from reading].

While observations from neuroimaging data are typically high-dimensional, it is commonly assumed that the data lie along a low-dimensional manifold [Relevant citations demonstrating this]. Under this assumption, manifold learning algorithms can be used to recover this underlying manifold and parameterize this high-dimensional input data in the learned low-dimensional space. Most commonly-used manifold learning algorithms, including Isomap (\cite{tenenbaumGlobalGeometricFramework2000}), Locally Linear Embedding (\cite{roweisNonlinearDimensionalityReduction2000}), Laplacian Eigenmaps (\cite{belkinLaplacianEigenmapsDimensionality2003}), and Diffusion Maps (\cite{coifmanDiffusionMaps2006}) construct a graph of similarities between observations, which is then used to determine the structure of the manifold. \cite{mengPrincipalManifoldEstimation2021} presents an alternative principal manifold framework for manifold estimation which extends to higher dimensions the principal curve framework introduced in \cite{hastiePrincipalCurves1989}, in which principal curves are defined as one-dimensional curves that pass through the middle of a high-dimensional dataset. 

In neuroimaging settings, manifold learning algorithms are most often used to estimate the underlying manifold at the population level, with vectorized image intensities being used as the input data [Citations]. \cite{yueParameterizationWhiteMatter2016} used manifold learning to estimate a low-dimensional parameterization of the corpus callosum at the individual level. However, in many cases, the outcome of interest is framed as a comparison between different groups of individuals (e.g. treatment groups in a clinical trial). This goal is particularly relevant in the case of the complex neurodegenerative conditions mentioned above, where the high degree of heterogeneity in patient experiences have led to attempts to identify subgroups of patients based on a range of criteria such as symptom expression, onset, or rate of progression [Citations]. Fundamentally, if a low dimensional manifold is thought to effectively represent data related to the patient experience of a neurodegenerative condition, and the experience of that condition differs by group, then the low-dimensional representation of that experience should also differ by group.

A method to accurately and efficiently estimate the manifolds underlying data at the group level must take advantage of the grouped structure of observations found in the dataset to which it is to be applied. To develop an estimate of the manifold underlying the hippocampi of participants in the ADNI study, it is necessary to recognize the basic nesting structure of the data, in which brain images are nested within participants, which may belong to a latent disease subgroup for which patients tend to have more related symptom experiences or underlying physiological developments, which are in turn nested within study groups based on Alzheimer's disease status, which are finally nested within the greater study population. The goal of this article is to develop a manifold learning approach that appropriately accounts for this hierarchical structure, estimates the underlying manifold at each level of the hierarchy, and allows for statistical testing of differences between those low-dimensional manifolds.

There have been several approaches taken to address hierarchical structure in manifold learning problems. \cite{bhatiaHierarchicalManifoldLearning2012} introduced a modification of Laplacian eigenmaps to estimate manifolds from recursively subdivided regions of medical images while enforcing similar alignment of the manifolds estimated from related regions. \cite{freicheCharacterizingMyocardialIschemia2022} builds on this approach to incorporate multiple imaging modalities and automate the tuning of the algorithm's hyperparameters. 

\cite{gaoHierarchicalManifoldLearning2019} proposed a two-step diffusion maps method for hierarchical data with variation over several domains, where diffusion maps is applied first to reduce the images for each individual to $d_1$ dimensions, resulting in individual-specific manifold estimates, then diffusion maps is applied again on the concatenated parameterizations to reduce over the individuals to $d_2$ dimensions, thus estimating a group-level manifold. This approach allows the first step of dimension reduction to capture the most important qualities of each image separately before reducing across subjects, but the two step procedure ensures that the manifold estimated at the individual level has no relation to the manifold estimated at the group level, posing problems for the interpretability of the embedding mappings and comparisons of manifolds between levels.

\cite{guerreroGroupconstrainedManifoldLearning2017} noted that the grouping behavior of observations recorded longitudinally from the same individual adversely impacted the creation of the neighborhood graph between data points when estimating a manifold for the entire population. The authors proposed a constraint specifying that, a given observation's collection of nearest neighbors may include at most one point from each individual or other specified grouping. This encourages a more even representation of the entire space of observations in the neighborhood graph, and may be applied to any manifold learning algorithm that features a neighborhood graph to specify proximities between observations. However, it simply mitigates the effects of the nesting structure on a population-level manifold estimate rather than enabling a detailed comparison of manifold estimates for each group.

This article builds on the principal manifold estimation (PME) algorithm introduced in \cite{mengPrincipalManifoldEstimation2021}, which uses an iterative procedure to specify an embedding mapping from the low-dimensional manifold to a high-dimensional space that takes the form of a smoothing spline. Drawing from literature in hierarchical and additive models, we replace the smoothing spline in the PME algorithm with an additive smoothing spline model (\cite{gelmanDataAnalysisUsing2007}, \cite{hastieGeneralizedAdditiveModels1990}, \cite{gelmanBayesianDataAnalysis2014}). While the specific model structure may change to meet the needs of the application, \cite{brumbackSmoothingSplineModels1998} and \cite{schulamFrameworkIndividualizingPredictions2015a} provide examples of the general model framework used. This ultimately yields a nonlinear dimension reduction algorithm that takes advantage of a statistically principled model to address nesting structures present in the input data. As it is based on the PME framework, the algorithm outputs an explicit representation of the embedding function, making it possible to both generate new data in high-dimensional space using low-dimensional parameters and map new input data to the estimated manifold. Smoothing spline analysis of variance allows the statistical comparison of the estimated manifolds between groups or individuals (\cite{wangMixedEffectsSmoothing1998}).

The following sections are organized as follows: Section 2 specifies an additive smoothing spline model for use in the analysis of neuroimaging data from the ADNI study, and provides a comprehensive description of the hierarchical PME algorithm developed in this paper. Section 3 applies this method to several simulated datasets using a variety of underlying manifold structures and tests performance on the MNIST dataset of handwritten digits. Section 4 demonstrates how the model specified in Section 2 may be used for the estimation of a manifold underlying the surfaces of subcortical structures of participants of the ADNI dataset, while Section 5 provides a concluding discussion of the proposed algorithm and opportunities for further study.

\section{Methods}

\subsection{Additive Smoothing Spline Model}

We begin this section with a brief review of additive models. Consider a regression scenario where output variable $\mathbf{y} = \left(y_1, \dots, y_n\right)^{T}$ are thought to be influenced by covariate vectors $\mathbf{x}_i = \left(x_{i1}, \dots, x_{ip}\right)$. In this situation, the standard multiple linear regression model assumes that the dependence of $y_i$ on $\mathbf{x}_i$ takes the linear form
\[%
  y_i = \alpha + x_{i1}\beta_1 + \dots + x_{ip}\beta_p + \epsilon_i
,\]%
where $\epsilon$ has mean 0 and variance $\sigma^2$. Additive models are a direct extension of this model, where the additive structure of the model is retained, but the assumption of linearity is discarded. A simple additive model takes the form
\[%
  y_i = \alpha + \sum_{j=1}^{p}f_j(X_{ij}) + \epsilon_i
,\]%
where again $\epsilon_i$ has mean 0 and variance $\sigma^2$, but the $f_j$s represent arbitrary functions which are commonly smooth. While the functions in the model above only take as parameters the value of one covariate, with each covariate being represented in the model once, this does not need to be the case. The structure of additive models seeks to balance benefits of interpretability found in additive models with the flexibility of nonlinear, and frequently, nonparametric functional forms. Further information can be found in \cite{hastieGeneralizedAdditiveModels1990}.

With this background in place, we turn to the motivating situation of analyzing the subcortical surfaces of ADNI participants using MR images, with a focus on the hippocampus. ADNI is a longitudinal observational study, where several imaging modalities are used to record neuroimages from participants over time. Participants can generally be considered to belong to three groups based on cognitive state: elderly healthy controls, those displaying symptoms of mild cognitive impairment, and those diagnosed with AD. In practice, participants may transition between these categorizations as their symptoms progress. However, because patients may be diagnosed with AD at different stages, accurately modeling these transitions is challenging and is beyond the scope of this article. For simplicity, we use the groupings recorded at the time each participant was recruited to the study. 

Individual participants are typically scheduled for imaging at six month intervals, though variability exists in terms of the recorded intervals. Also of note, the numbers of observations contributed by each participant varies, so groups may be unbalanced. We denote the total number of participants as $N$, and use $n_i$ to represent the number of images present for each individual. Meanwhile, $G$ denotes the number of observed groups under consideration.

Within a segmented structural MRI, the voxels estimated to mark the surface of the hippocampus can be assumed to lie along a two-dimensional manifold that has been wrapped into the three-dimensional image space. More generally, we let $d$ denote the intrinsic dimension of the low-dimensional manifold, and $D$ represent the dimension of the high-dimensional input space. For the $j$th image from the $i$th individual belonging to group $g$ with $i = 1, \dots, N$, $j = 1, \dots, n_i$, and $g = 1, \dots, G$, we denote the $k$th voxel in the high-dimensional space as $\mathbf{x}_{ijk} \in \mathbb{R}^{3}$, and its associated low-dimensional parameterization as $\mathbf{y}_{ijk} \in \mathbb{R}^2$. We let $t_{ij}$ denote the time at which the $j$th image for individual $i$ is recorded. The embedding function which maps from the $d$-dimensional manifold to $D$-dimensional ambient space is represented as $\psi(\mathbf{y}_{ijk})$, which can be modeled as \[%
  \mathbf{x}_{ijk} = \psi(\mathbf{y}_{ijk}) = f_p(\mathbf{y}_{ijk}, t_{ij}) + f_{g(i)}(\mathbf{y}_{ijk}, t_{ij}) + f_i(\mathbf{y}_{ijk}, t_{ij}) + f_{ij}(\mathbf{y}_{ijk}) + \epsilon
.\]%

Each function in this additive model represents a different level of nesting in the data, with $f_p(\mathbf{y}, t)$ estimating the embedding at the population level, and $f_g(\mathbf{y}, t)$, $f_i(\mathbf{y}, t)$, and $f_{ij}(\mathbf{y})$ describing deviations from the population estimates specific to the group, individual, and image in question, respectively. As estimating longitudinal changes in the underlying manifold may be of interest, $f_p(\mathbf{y}, t)$, $f_g(\mathbf{y}, t)$ and $f_i(\mathbf{y}, t)$ are each denoted as varying-coefficient models (\cite{hastieVaryingCoefficientModels1993}). Thus, the model is capable of estimating separate longitudinal changes for population, group, and individual estimates. In cases where longitudinal changes are not of interest, each varying-coefficient model may be constrained to be held constant with respect to time. As the $ij$th image is recorded at a single time point, $f_{ij}(\mathbf{y}, t)$ may not be dependent on time. We specify $f_{ij}(\mathbf{y})$ as a smoothing spline, while the functions $f_p(\mathbf{y}, t)$, $f_g(\mathbf{y}, t)$, and $f_i(\mathbf{y}, t)$ are each varying-coefficient smoothing spline models, estimated by using a smoothing spline to smooth over smoothing spline coefficients calculated at each time point available in the data.

As mentioned previously, this proposed additive spline model structure has several benefits. First, it maintains interpretability at each level of the model. It is possible to visually inspect the embedding maps of the population and each group and individual to better understand how they interact. Additionally, basing our handling of the data's hierarchical structure on a statistical model allows us to assess statistical properties of the estimated manifold, where less statistically principled approaches make this option difficult.

\subsection{Hierarchical Principal Manifold Estimation}

\section{Application to Simulated and MNIST Data}

\subsection{Application to Simulated Data}

\subsection{Application to MNIST Data}

\section{Application to ADNI Data}

\section{Discussion}

\newpage

\nocite{*}
%\bibliographystyle{plain}
%\bibliography{references}
\printbibliography

\end{document}
