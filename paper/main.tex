%\documentclass[12pt]{amsart}
\documentclass[11pt,reqno]{article}
%\usepackage{cases}

%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
\RequirePackage{graphicx}%% uncomment this for including figures

\usepackage[
  backend=biber,
  style=authoryear
]{biblatex}

\addbibresource{references.bib}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage[perpage,symbol*]{footmisc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}  
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm2e}

%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={(},close={)}}
\usepackage{authblk}

\usepackage{csquotes}
\usepackage[english]{babel}

\renewcommand{\baselinestretch}{1.0}
\setlength{\oddsidemargin}{-0.5cm}
\setlength{\evensidemargin}{-0.5cm}
\renewcommand{\topmargin}{-2cm}
\renewcommand{\oddsidemargin}{0mm}
\renewcommand{\evensidemargin}{0mm}
\renewcommand{\textwidth}{180mm}
\renewcommand{\textheight}{240mm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\T}{\intercal}
\newcommand{\commentout}[1]{}

\newcommand{\kmedit}[1]{{\color{purple}  #1}}
\newcommand{\meng}[1]{{\color{purple} \sf $\clubsuit\clubsuit\clubsuit$ Kun Meng: [#1]}}
\newcommand{\Meng}[1]{\margMa{(Kun Meng) #1}}

\newcommand{\zielinski}[1]{{\color{blue} \sf $\spadesuit\spadesuit\spadesuit$ Rob Zielinski: [#1]}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\begin{document}

\title{Hierarchical Principal Manifold Estimation}
\author[1]{Robert Zielinski}
\author[2]{Kun Meng}
\author[1]{Ani Eloyan}
\affil[1]{Department of Biostatistics, Brown University}
\affil[2]{Division of Applied Mathematics, Brown University}



\maketitle

\doublespacing

\section*{Abstract}

\section{Introduction}

Neurodegenerative diseases such as Alzheimer's disease (AD) and Parkinson's disease are highly complex, chronic conditions that exhibit substantial heterogeneity between patients. To enable researchers to identify potential biomarkers of progression for these conditions, large multi-site longitudinal observational studies have been conducted that collect information from a wide range of participants over long durations across several modalities, including neuroimaging [Cite ADNI and PPMI studies]. Neuroimaging data in particular have been shown to be sensitive to developments in the early stages of disease progression, often before diagnosis occurs [Find citation from reading].

While observations from neuroimaging data are typically high-dimensional, it is commonly assumed that the data lie along a low-dimensional manifold [Relevant citations demonstrating this]. Under this assumption, manifold learning algorithms can be used to recover this underlying manifold and parameterize this high-dimensional input data in the learned low-dimensional space. Most commonly-used manifold learning algorithms, including Isomap (\cite{tenenbaumGlobalGeometricFramework2000}), Locally Linear Embedding (\cite{roweisNonlinearDimensionalityReduction2000}), Laplacian Eigenmaps (\cite{belkinLaplacianEigenmapsDimensionality2003}), and Diffusion Maps (\cite{coifmanDiffusionMaps2006}) construct a graph of similarities between observations, which is then used to determine the structure of the manifold. \cite{mengPrincipalManifoldEstimation2021} presents an alternative principal manifold framework for manifold estimation which extends to higher dimensions the principal curve framework introduced in \cite{hastiePrincipalCurves1989}, in which principal curves are defined as one-dimensional curves that pass through the middle of a high-dimensional dataset. 

In neuroimaging settings, manifold learning algorithms are most often used to estimate the underlying manifold at the population level, with vectorized image intensities being used as the input data [Citations]. \cite{yueParameterizationWhiteMatter2016} used manifold learning to estimate a low-dimensional parameterization of the corpus callosum at the individual level. However, in many cases, the outcome of interest is framed as a comparison between different groups of individuals (e.g. treatment groups in a clinical trial). This goal is particularly relevant in the case of the complex neurodegenerative conditions mentioned above, where the high degree of heterogeneity in patient experiences have led to attempts to identify subgroups of patients based on a range of criteria such as symptom expression, onset, or rate of progression [Citations]. Fundamentally, if a low dimensional manifold is thought to effectively represent data related to the patient experience of a neurodegenerative condition, and the experience of that condition differs by group, then the low-dimensional representation of that experience should also differ by group.

A method to accurately and efficiently estimate the manifolds underlying data at the group level must take advantage of the grouped structure of observations found in the dataset to which it is to be applied. To develop an estimate of the manifold underlying the hippocampi of participants in the ADNI study, it is necessary to recognize the basic nesting structure of the data, in which brain images are nested within participants, which may belong to a latent disease subgroup for which patients tend to have more related symptom experiences or underlying physiological developments, which are in turn nested within study groups based on Alzheimer's disease status, which are finally nested within the greater study population. The goal of this article is to develop a manifold learning approach that appropriately accounts for this hierarchical structure, estimates the underlying manifold at each level of the hierarchy, and allows for statistical testing of differences between those low-dimensional manifolds.

There have been several approaches taken to address hierarchical structure in manifold learning problems. \cite{bhatiaHierarchicalManifoldLearning2012} introduced a modification of Laplacian eigenmaps to estimate manifolds from recursively subdivided regions of medical images while enforcing similar alignment of the manifolds estimated from related regions. \cite{freicheCharacterizingMyocardialIschemia2022} builds on this approach to incorporate multiple imaging modalities and automate the tuning of the algorithm's hyperparameters. 

\cite{gaoHierarchicalManifoldLearning2019} proposed a two-step diffusion maps method for hierarchical data with variation over several domains, where diffusion maps is applied first to reduce the images for each individual to $d_1$ dimensions, resulting in individual-specific manifold estimates, then diffusion maps is applied again on the concatenated parameterizations to reduce over the individuals to $d_2$ dimensions, thus estimating a group-level manifold. This approach allows the first step of dimension reduction to capture the most important qualities of each image separately before reducing across subjects, but the two step procedure ensures that the manifold estimated at the individual level has no relation to the manifold estimated at the group level, posing problems for the interpretability of the embedding mappings and comparisons of manifolds between levels.

\cite{guerreroGroupconstrainedManifoldLearning2017} noted that the grouping behavior of observations recorded longitudinally from the same individual adversely impacted the creation of the neighborhood graph between data points when estimating a manifold for the entire population. The authors proposed a constraint specifying that, a given observation's collection of nearest neighbors may include at most one point from each individual or other specified grouping. This encourages a more even representation of the entire space of observations in the neighborhood graph, and may be applied to any manifold learning algorithm that features a neighborhood graph to specify proximities between observations. However, it simply mitigates the effects of the nesting structure on a population-level manifold estimate rather than enabling a detailed comparison of manifold estimates for each group.

This article builds on the principal manifold estimation (PME) algorithm introduced in \cite{mengPrincipalManifoldEstimation2021}, which uses an iterative procedure to specify an embedding mapping from the low-dimensional manifold to a high-dimensional space that takes the form of a smoothing spline. Drawing from literature in hierarchical and additive models, we replace the smoothing spline in the PME algorithm with an additive smoothing spline model (\cite{gelmanDataAnalysisUsing2007}, \cite{hastieGeneralizedAdditiveModels1990}, \cite{gelmanBayesianDataAnalysis2014}). While the specific model structure may change to meet the needs of the application, \cite{brumbackSmoothingSplineModels1998} and \cite{schulamFrameworkIndividualizingPredictions2015a} provide examples of the general model framework used. This ultimately yields a nonlinear dimension reduction algorithm that takes advantage of a statistically principled model to address nesting structures present in the input data. As it is based on the PME framework, the algorithm outputs an explicit representation of the embedding function, making it possible to both generate new data in high-dimensional space using low-dimensional parameters and map new input data to the estimated manifold. Smoothing spline analysis of variance allows the statistical comparison of the estimated manifolds between groups or individuals (\cite{wangMixedEffectsSmoothing1998}).

The following sections are organized as follows: Section 2 specifies an additive smoothing spline model for use in the analysis of neuroimaging data from the ADNI study, and provides a comprehensive description of the hierarchical PME algorithm developed in this paper. Section 3 applies this method to several simulated datasets using a variety of underlying manifold structures and tests performance on the MNIST dataset of handwritten digits. Section 4 demonstrates how the model specified in Section 2 may be used for the estimation of a manifold underlying the surfaces of subcortical structures of participants of the ADNI dataset, while Section 5 provides a concluding discussion of the proposed algorithm and opportunities for further study.

\section{Methods}

\subsection{Additive Smoothing Spline Model}

We begin this section with a brief review of additive models. Consider a regression scenario where output variable $\mathbf{y} = \left(y_1, \dots, y_n\right)^{T}$ are thought to be influenced by covariate vectors $\mathbf{x}_i = \left(x_{i1}, \dots, x_{ip}\right)$. In this situation, the standard multiple linear regression model assumes that the dependence of $y_i$ on $\mathbf{x}_i$ takes the linear form
\[%
  y_i = \alpha + x_{i1}\beta_1 + \dots + x_{ip}\beta_p + \epsilon_i
,\]%
where $\epsilon$ has mean 0 and variance $\sigma^2$. Additive models are a direct extension of this model, where the additive structure of the model is retained, but the assumption of linearity is discarded. A simple additive model takes the form
\begin{equation}
  y_i = \alpha + \sum_{j=1}^{p}f_j(X_{ij}) + \epsilon_i, \label{eq:1}
\end{equation}
where again $\epsilon_i$ has mean 0 and variance $\sigma^2$, but the $f_j$s represent arbitrary functions which are commonly smooth. While the functions in model \ref{eq:1} only take as parameters the value of one covariate, with each covariate being represented in the model once, this does not need to be the case. The structure of additive models seeks to balance benefits of interpretability found in additive models with the flexibility of nonlinear, and frequently, nonparametric functional forms. Further information can be found in \cite{hastieGeneralizedAdditiveModels1990}.

With this background in place, we turn to the motivating situation of analyzing the subcortical surfaces of ADNI participants using MR images, with a focus on the hippocampus. ADNI is a longitudinal observational study, where several imaging modalities are used to record neuroimages from participants over time. Participants can generally be considered to belong to three groups based on cognitive state: elderly healthy controls, those displaying symptoms of mild cognitive impairment, and those diagnosed with AD. In practice, participants may transition between these categorizations as their symptoms progress. However, because patients may be diagnosed with AD at different stages, accurately modeling these transitions is challenging and is beyond the scope of this article. For simplicity, we use the groupings recorded at the time each participant was recruited to the study. 

Individual participants are typically scheduled for imaging at six month intervals, though variability exists in terms of the recorded intervals. Also of note, the numbers of observations contributed by each participant varies, so groups may be unbalanced. We use $G$ to denote the number of observed groups under consideration, then denote the number of individuals as $N$, the number of images belonging to the $i$th individual as $n_i$, and the number of voxels from the $j$th image of individual $i$ as $m_{ij}$. 

Within a segmented structural MRI, the voxels estimated to mark the surface of the hippocampus can be assumed to lie along a two-dimensional manifold that has been wrapped into the three-dimensional image space. More generally, we let $d$ denote the intrinsic dimension of the low-dimensional manifold, and $D$ represent the dimension of the high-dimensional input space. For the $j$th image from the $i$th individual belonging to group $g$ with $i = 1, \dots, N_g$, $j = 1, \dots, n_i$, and $g = 1, \dots, G$, we denote the $k$th voxel in the $D$-dimensional space as $\mathbf{x}_{ijk} \in \mathbb{R}^{D}$, its associated $d$-dimensional parameterization as $\mathbf{y}_{ijk} \in \mathbb{R}^{d}$, and its weight as $w_{ijk}$. We let $t_{ij}$ denote the time at which the $j$th image for individual $i$ is recorded, with all times measured as the difference between the date the image was taken and the date of the participant's baseline visit. We use $\mathbf{x}_g$, $\mathbf{x}_i$, $\mathbf{x}_{ij}$, and $\mathbf{x}_t$ to denote the input voxels from all images in the $g$th group, $i$th individual, $ij$th image, and $t$th time point, respectively, and use similar notation for all other relevant terms.

The embedding function which maps from the $d$-dimensional manifold to $D$-dimensional ambient space is represented as $\psi(\mathbf{y}_{ijk})$, which can be modeled as 
\begin{equation}
  \mathbf{x}_{ijk} = \psi(\mathbf{y}_{ijk}) = f_p(\mathbf{y}_{ijk}, t_{ij}) + f_{g(i)}(\mathbf{y}_{ijk}, t_{ij}) + f_i(\mathbf{y}_{ijk}, t_{ij}) + f_{ij}(\mathbf{y}_{ijk}) + \epsilon. \label{eq:2}
\end{equation}
Each function in this additive model represents a different level of nesting in the data, with $f_p(\mathbf{y}, t)$ estimating the embedding at the population level, and $f_g(\mathbf{y}, t)$, $f_i(\mathbf{y}, t)$, and $f_{ij}(\mathbf{y})$ describing deviations from the population estimates specific to the group, individual, and image in question, respectively. As estimating longitudinal changes in the underlying manifold may be of interest, $f_p(\mathbf{y}, t)$, $f_g(\mathbf{y}, t)$ and $f_i(\mathbf{y}, t)$ are each denoted as varying-coefficient models, where the coefficients of each function are dependent on time (\cite{hastieVaryingCoefficientModels1993}). Thus, the model is capable of estimating separate longitudinal changes for population, group, and individual estimates. In cases where longitudinal changes are not of interest, each varying-coefficient model may be constrained to be held constant with respect to time. As the $ij$th image is recorded at a single time point, $f_{ij}(\mathbf{y}, t)$ may not be dependent on time. We specify $f_{ij}(\mathbf{y})$ as a smoothing spline, while the functions $f_p(\mathbf{y}, t)$, $f_g(\mathbf{y}, t)$, and $f_i(\mathbf{y}, t)$ are each varying-coefficient smoothing spline models, estimated by using a smoothing spline to smooth over smoothing spline coefficients calculated at each time point available in the data. 

The smoothing spline functions $f_{ij}$ minimize 
\begin{equation}
  \mathcal{K}_{\lambda}(f_{ij}) = \sum_{l=1}^{D}\left\{\sum_{k=1}^{m_{ij}}w_{ijk}|\mathbf{x}_{ijk, l} - f_{ij, l}(\mathbf{y}_{ijk})|^2 + \lambda\|\nabla^{\otimes 2}f_l\|^2\right\}, \label{eq:3}
\end{equation}
where $\nabla^{\otimes 2}f$ denotes the Hessian matrix of $f$, and $\lambda$ is given. Thus, $f_{ij}$ takes the form
\begin{equation}
  f_{ij, l}(\mathbf{y}) = \sum_{k=1}^{m_{ij}} s_{k, l} \times \eta_{4-d}\left(\mathbf{y} - \mathbf{y}_{ijk}\right) + \sum_{p=1}^{d+1} \alpha_{p, l} \times \phi_p(\mathbf{y}), \quad l = 1, 2, \dots, D, \label{eq:4}
\end{equation}
with $\phi_p(\mathbf{y})$ representing the basis of the linear space of polynomials on $\mathbb{R}^{d}$ with degree $\leq 1$ and $\eta_{\nu}(\mathbf{y}) = \|\mathbf{y}\|_{\mathbb{R}^{d}}^{\nu}\log\left(\|\mathbf{y}\|_{\mathbb{R}^{d}}\right)$ when $\|\mathbf{y}\|_{\mathbb{R}^{d}} \neq 0$ and $\nu$ is even, $\eta_{\nu}(\mathbf{y}) = 0$ when $\|\mathbf{y}\|_{\mathbb{R}^{d}} = 0$ and $\nu$ is even, and $\eta_{\nu}(\mathbf{y}) = \|\mathbf{y}\|_{\mathbb{R}^{d}}^{\nu}$ when $\nu$ is odd. 

Without accounting for the additive structure of the model, and given $m_{ij}$ input values $\mathbf{y}_{ij}$, weights $\mathbf{w}_{ij}$, and outputs $x_{ij}$, we can estimate $f_{ij}$ by obtaining the values of $s_l \in \mathbb{R}^{m_{ij}}$ and $\alpha_l \in \mathbb{R}^{d + 1}$ for $l = 1, \dots, D$ that minimize $\|\mathbf{W}^{\frac{1}{2}}\left(\mathbf{x}_{ij, l} - \mathbf{E}s_l - \mathbf{R}\alpha_l\right)\|^2_{\mathbb{R}^{m_{ij}}} + \lambda\|\mathbf{E}^{\frac{1}{2}}s_l\|^2_{\mathbb{R}^{m_{ij}}}$ under the constraint that $\mathbf{R}^{T}s_l = 0$, where
\begin{itemize}
  \item $\mathbf{E} = (E_{qr})_{1 \leq q, r, \leq m_{ij}}$ is defined by $E_{qr} = \eta_{4-d}(\mathbf{y}_{ijq} - \mathbf{y}_{ijr})$.
  \item $\mathbf{R} = \left(R_{kp}\right)_{1 \leq k \leq m_{ij}, \ 1 \leq p \leq d + 1}$ is defined by $R_{kp} = \phi_p(\mathbf{y}_{ijk})$.
  \item $\mathbf{W} = diag(w_{ij1}, \dots, w_{ijm_{ij}})$.
\end{itemize}

With $m_l$ representing Lagrange multipliers, the solutions for $s_l$ and $\alpha_l$ can be found by solving

\begin{equation}
   \left(\begin{array}{ccc}
    2\mathbf{EWE} + 2\lambda\mathbf{E} & 2\mathbf{EWR} & \mathbf{R} \\
    2\mathbf{R}^{T}\mathbf{WE} & 2\mathbf{R}^{T}\mathbf{WR} & \mathbf{0} \\
    \mathbf{R}^{T} & \mathbf{0} & \mathbf{0}
  \end{array}\right)\left(
  \begin{array}{c}
    s_l \\
    \alpha_l \\
    m_l
  \end{array}
  \right) = \left(
  \begin{array}{c}
    2\mathbf{EW}\mathbf{x}_l \\
    2\mathbf{R}^{T}\mathbf{W}\mathbf{x}_l \\
    \mathbf{0}
  \end{array}
  \right). \label{eq:5}
\end{equation}
The optimal value of $\lambda$ can be found using cross-validation, with the functions being evaluated by the mean squared error, written as
\begin{equation}
  MSE(\lambda) = \frac{1}{m_{ij}}\sum_{k=1}^{m_{ij}}\left(\mathbf{x}_{ijk} - f_{ij}(\mathbf{y}_{ijk})\right)^2. \label{eq:6}
\end{equation}

The solutions to this minimization problem suffice for estimating $f_{ij}(\mathbf{y})$, in which the coefficients are not time-varying. To reach estimates for the varying-coefficient models, equation \ref{eq:5} is solved using the data available at each time point within the relevant group. To estimate $f_p(\mathbf{y}, t)$, we use $\left\{\mathbf{x}_t\right\}_{t=1}^T$ and $\left\{\mathbf{y}_t\right\}_{t=1}^T$, yielding fitted function $\left\{\hat{f}_t(\mathbf{y})\right\}_{t=1}^T$. Using the maximum and minimum values of $\mathbf{y}_t$ in each dimension, a new $d$-dimensional grid of $m' = \text{max}\left(\left\{m_{ij}\right\}_{i=1, j = 1}^{N, N_i}\right)$ knots, $\left\{\mathbf{y}'\right\}$, is generated with the knots being evenly spread throughout the parameter space. These parameters serve as common knots that allow the spline coefficients to be compared across time points. They are then used to compute a new set of corresponding outcome values $\left\{\mathbf{x}'_t\right\}_{t=1}^{T}$. The spline functions are reestimated as $\left\{\hat{f}'_t\right\}_{t=1}^{T}$ using $\left\{\mathbf{x}'_t\right\}_{t=1}^{T}$ and $\mathbf{y}'$. These newly generated knots are weighted equally, so an unweighted smoothing spline model is used. Following \cite{greenSilverman1994}, with

\begin{itemize}
  \item $\mathbf{E}' = \left(E'_{qr}\right)_{1 \leq q, r, \leq m'}$ defined by $E'_{qr} = \eta_{4-d}\left(\mathbf{y}'_{q} - \mathbf{y}'_{r}\right)$.
  \item $\mathbf{R}' = \left(R'_{qp}\right)_{1 \leq q \leq m', \ 1 \leq p \leq d + 1}$ is defined by $R'_{qp} = \phi_p(\mathbf{y}'_{q})$
\end{itemize}
the coefficients that minimize $\left(\mathbf{x}'_t - \mathbf{E}'\mathbf{s}_t - \mathbf{R}'^{T}\mathbf{\alpha}_t\right)^{T}\left(\mathbf{x}'_t - \mathbf{E}'\mathbf{s}_t - \mathbf{R}'^{T}\mathbf{\alpha}_t\right) + \gamma\mathbf{s}_t^{T}\mathbf{E}'\mathbf{s}_t$ can be found by solving 
\begin{equation}
  \left(
  \begin{array}{cc}
    \mathbf{E}' + \lambda_{t}\mathbf{I} & \mathbf{R}'^{T} \\
    \mathbf{R}' & \mathbf{0}
  \end{array}
  \right)\left(
  \begin{array}{c}
    \mathbf{s}'_t \\
    \mathbf{\alpha}'_t
  \end{array}
  \right) = \left(
  \begin{array}{c}
    \mathbf{x}'_t \\
    \mathbf{0}
  \end{array}
  \right). \label{eq:7}
\end{equation}

The coefficients found at each time point $t$, $\mathbf{s'}_{t}$ and $\mathbf{\alpha'}_{t}$ are concatenated in matrix $\mathbf{\omega}_{t} = \left(\mathbf{s'}_{t}^T, \mathbf{\alpha'}_{t}^T\right)$. This coefficient matrix is then used as the output value in a cubic smoothing spline function using the time points as input values. This function takes the form
\begin{equation}
  g_{\gamma}(t) = \sum_{i=1}^{T}\delta_i\|t - t_i\|^{3} + \sum_{p=1}^{2}\tau_p \phi(t)_p. \label{eq:8}
\end{equation}
We define matrix $\mathbf{A}$ as $\mathbf{A}_{qr} = \|t_q - t_r\|^{3}$, matrix $\mathbf{T}$ as $\mathbf{T}_{pr} = \phi_p(t_r)$, $\mathbf{\delta} = \left(\delta_1, \dots, \delta_T\right)^{T}$, and $\mathbf{\tau} = \left(\tau_1, \tau_2\right)^{T}$. Then, the coefficients of $g_{\gamma}(t)$, given a value of $\gamma$, can be found by solving
\begin{equation}
  \left(
  \begin{array}{cc}
    \mathbf{A} + \gamma\mathbf{I} & \mathbf{T}^{T} \\
    \mathbf{T} & \mathbf{0}
  \end{array}
  \right)\left(
  \begin{array}{c}
    \mathbf{\delta} \\
    \mathbf{\tau}
  \end{array}
  \right) = \left(
  \begin{array}{c}
    \mathbf{\omega} \\
    \mathbf{0}
  \end{array}
  \right). \label{eq:9}
\end{equation}

As with the tuning process for $\lambda$, the optimal value of $\gamma$ can be identified using cross-validation, with either leave-one-out or $k$-folds cross-validation being appropriate depending on the number of time points available. With a proper estimate of the optimum $\gamma$, we may denote the coefficients estimated by $g(t)$ as $g(t) = \Omega(t) = \left(\mathbf{s}_{\gamma}(t)^{T}, \mathbf{\alpha}_{\gamma}(t)^{T}\right)^{T}$. Therefore, the varying-coefficient models have the following outputs:
\begin{equation}
  f(\mathbf{y}, t) = \sum_{k=1}^{m'}\mathbf{s}_{\gamma}(t)_l \eta_{4-d}\left(\|\mathbf{y} - \mathbf{y'}_k\|\right) + \sum_{p=1}^{d + 1}\mathbf{\alpha}_{\gamma}(t)_p \phi_p(\mathbf{y}). \label{eq:10}
\end{equation}
The fitting process for the varying-coefficient smoothing spline model is given formally as Algorithm \ref{alg:varying_spline}.

\SetKwComment{Comment}{/* }{ */}

\RestyleAlgo{ruled}
\LinesNumbered

\begin{algorithm}
\caption{Varying-Coefficient Smoothing Spline Model}\label{alg:varying_spline}
  \KwData{$D$-dimensional output values $\left\{\mathbf{x}_{ijk}\right\}_{i=1, j = 1, k = 1}^{N, n_i, m_{ij}}$, $d$-dimensional input values $\left\{\mathbf{y}_{ijk}\right\}_{i=1, j=1, k=1}^{N, n_i, m_{ij}}$, weights $\left\{w_{ijk}\right\}_{i=1, j=1, k=1}^{N, n_i, m_{ij}}$, observation times $\left\{t_{ij}\right\}_{i=1, j=1}^{N, n_i}$, candidate tuning parameters $\left\{\lambda_l\right\}_{l=1}^{L}$, $\left\{\gamma_p\right\}_{p=1}^{P}$.}
  \KwResult{Analytic formula of $\hat{f}(\mathbf{y}, t)$ and optimal tuning parameter $\hat{\gamma}$.}

\For{$t = 1, 2, \dots, T$} {
  Set $U_t$ equal to the number of images included in the data at time $t$\;
  \For{$l = 1, 2, \dots, L$} {
    Solve equation \ref{eq:5} to estimate $f_{\lambda_l, t}(\mathbf{y})$ with inputs $\mathbf{x}_t$, $\mathbf{y}_t$, and $w_t$\;
    \For{$u = 1, 2, \dots, U_t$} {
      Solve equation \ref{eq:5} to estimate $f^{(-u)}_{\lambda_l, t}(\mathbf{y})$ with inputs $\mathbf{x}^{(-u)}_t$, $\mathbf{y}^{(-u)}_t$, $w^{(-u)}_t$\;
      Estimate $MSE(\lambda_{l})_u$ using equation \ref{eq:6}\;
    }
    Set $MSE(\lambda_l) = \frac{1}{U}\sum_{u=1}^{U}MSE(\lambda_l)_u$\;
  }
  $\lambda_t = \arg \min_{\lambda} MSE(\lambda)$\;
  $f_t(\mathbf{y}) = f_{\lambda_t}(\mathbf{y})$\;
}

  Let $m' = \max(m_{ij})$, $\left\{\mathbf{y}'_{k}\right\}_{k=1}^{m'}$ be a grid spanning the range of values of $\left\{\mathbf{y}\right\}$\;
  \For{$t = 1, 2, \dots, T$} {
    Compute $\left\{\mathbf{x'}_{k, t}\right\}_{k=1}^{m'} = \left\{f_t(\mathbf{y'}_k)\right\}$\;
    Solve equation \ref{eq:7} to compute $\mathbf{s'}_t$ and $\mathbf{\alpha'}_t$\;
    Set $\mathbf{\omega}_t = \left[\mathbf{s'}_t^{T}, \mathbf{\alpha'}_t^{T}\right]$\;
  }
  \For{$p = 1, 2, \dots, P$} {
    Compute $g_{\gamma_l}(t)$ by solving equation \ref{eq:9}\;
    \For{$t = 1, 2, \dots, T$} {
      Compute $g_{\gamma_l}^{(t)}$ by solving \ref{eq:9}\;
    }
    Estimate $MSE(\gamma_l)$ using \ref{eq:10}\;
  }
  $\hat{\gamma} = \arg \min_{\gamma} MSE(\gamma)$\;
  $\hat{f}(\mathbf{y}, t) = f_{\hat{\gamma}}(\mathbf{y}, t)$, where the form of $\hat{f}(\mathbf{y}, t)$ is given in \ref{eq:11}
\end{algorithm}



As mentioned previously, this proposed additive spline model structure has several benefits. First, it maintains interpretability at each level of the model. It is possible to visually inspect the embedding maps of the population and each group and individual to better understand how they interact. While the model described here is intended to be applicable to the specific situation involving the modeling of data from the ADNI study, this general modeling approach can be conveniently adapted to a variety of scenarios. Additionally, basing our handling of the data's hierarchical structure on a statistical model allows us to assess statistical properties of the estimated manifold, where less statistically principled approaches make this option difficult. Of particular interest, \cite{wangMixedEffectsSmoothing1998} and \cite{brumbackSmoothingSplineModels1998} explore connections between smoothing spline models and mixed-effects models. The smoothing spline analysis of variance proposed by \cite{wangMixedEffectsSmoothing1998} may be applicable to the model structure proposed here, which would enable a detailed decomposition of the variation in manifold structure by level. For the time being, this extension is left to future work.

As discussed in \cite{hastieGeneralizedAdditiveModels1990}, the additive model $\psi(\mathbf{y})$ can be estimated using a backfitting procedure, which uses the model's additive nature to iteratively fit each distinct smoothing spline function until convergence. Specifically, we use the fact that, under the model,

\[%
f_p(\mathbf{y}_{ijk}, t_{ij}) = E\left[\mathbf{x}_{ijk} - f_{g(i)}(\mathbf{y}_{ijk}, t_{ij}) - f_{i}(\mathbf{y}_{ijk}, t_{ij}) - f_{ij}(\mathbf{y}_{ijk})\right]
,\]%
with a similar result holding for each of the smooth functions making up the additive model. Thus, the backfitting algorithm to fit the proposed model is given below.

\zielinski{The following algorithm is currently used as a placeholder while I work out the specifics.}

\zielinski{Try using a ratio of the norm of the difference in model coefficients and the norm of the old model to measure convergence. This will set the change in the model coefficients (as measured by the norm of the difference vector/matrix) relative to the magnitude of the model coefficients.}

\begin{algorithm}
\caption{Backfitting Algorithm for Additive Spline Model}\label{alg:backfitting}
  \KwData{$D$-dimensional output values $\left\{\mathbf{x}_{ijk}\right\}_{i=1, j=1, k=1}^{N, n_i, m_{ij}}$, $d$-dimensional input values $\left\{\mathbf{y}_{ijk}\right\}_{i=1, j=1, k=1}^{N, n_i, m_{ij}}$, and observation times $\left\{t_{ij}\right\}_{i=1, j=1}^{N, n_i}$.}
  \KwResult{Analytic formulas of $\hat{f}_p$, $\left\{\hat{f}_g\right\}_{g=1}^{G}, \left\{\hat{f}_i\right\}_{i=1}^{N}$, and $\left\{\hat{f}_{ij}\right\}_{i=1, j=1}^{N, n_i}$, and optimal tuning parameters $\hat{\lambda}_p$, $\hat{\gamma}_p$, $\left\{\hat{\lambda}_g\right\}_{g=1}^{G}$, $\left\{\hat{\gamma}\right\}_{g=1}^{G}$, $\left\{\hat{\lambda}_{i}\right\}_{i=1}^{N}$, $\left\{\hat{\gamma}_{i}\right\}_{i=1}^{N}$, $\left\{\hat{\lambda}_{ij}\right\}_{i=1, j=1}^{N, n_i}$.}

\For{$t = 1, 2, \dots, T$} {
  Apply HDMDE algorithm with input $\left(\left\{Y_{i, t}\right\}_{i = 1}^{I_t}, N_0, \epsilon, \alpha\right)$ and obtain $N_t$, $\left\{\mu_{j, t}\right\}_{j = 1}^{N_t}$, and $\left\{\theta_{j, t}\right\}_{j = 1}^{N_t}$\;
}

  Apply Isomap to parameterize $\left\{\mu_{j, t}\right\}_{j = 1, t = 1}^{N_t, T}$ by the $d$-dimensional parameters $\left\{r_{j, t}\right\}_{j = 1, t = 1}^{N_t, T}$. Formally set $\pi_{f_t(0)}(\mu_{j, t}) \gets r_{j, t}$ for $j = 1, 2, \dots, N_t$, $t = 1, 2, \dots, T$\;

  
\For{t = 1, 2, \dots, T} {
  Apply modified PME algorithm with $\pi_{f_t(0)}(\mu_{j, N_t, t}) \gets \left\{r_{j, t}\right\}_{j = 1}^{N_t}$ and obtain $f_t$, $\lambda_t^*$, and $\tau_t$\;
  $\left\{r_{j, t}\right\} \gets \pi_{f_t}(\mu_{j, t})$ for $j = 1, \dots, N_t$ and $X_{j, t} \gets f_t(r_{j, t})$ for $j = 1, \dots, N_t$\;
}

  Let $N = \max(N_t)$, $\left\{r_i^*\right\}_{i=1}^{N}$ be a grid spanning the range of estimated values of $\left\{r_{i, j}\right\}_{i=1, t=1}^{N_t, T}$\; 

\For{t = 1, 2, \dots, T} {
  Set $X_{i, t} = f_t(r_{i}^*)$ for $i = 1, \dots, N$\;
  Compute $f_t^*(r)$ by solving \eqref{eq:13}\;
  Set $w_t = \frac{\frac{1}{\tau_t}}{\sum_{i=1}^{T}\frac{1}{\tau_i}}$\;
}

  Define $\boldsymbol{\omega}$ by setting $\boldsymbol{\omega}_t = \left[\boldsymbol{s}_t, \boldsymbol{\alpha}_t\right]$\;
  \For{l = 1, 2, \dots, L} {
    Compute $g_{\gamma_l}(t)$ by solving \eqref{eq:16}\;
    \For{t = 1, 2, \dots, T} {
      Compute $g_{\gamma_l}^{(t)}$ by solving \eqref{eq:16}\;
    }
    Estimate $MSD(\gamma_l)$ using \eqref{eq:18}\;
  }
  $\gamma^* = \arg\min_{\gamma}MSD(\gamma)$\;
  $f^*(t, \boldsymbol{r}) = f_{\gamma^*}(t, \boldsymbol{r})$, where the form of $f^*(t, \boldsymbol{r})$ is given in \eqref{eq:17}
\end{algorithm}

A clear concern with the use of this procedure in this case is the number of functions to be fit, which can quickly increase the computational burden of fitting the models. However, the nesting of the different levels of the data helps to limit the number of function combinations that must be fit, helping to keep computational requirements manageable. \cite{brumbackSmoothingSplineModels1998} used restricted maximum likelihood estimation to reach an estimate of their proposed additive spline model, which may potentially provide computational improvements. However, in the case that the model should be modified to meet the needs of a new dataset, the backfitting algorithm will be more easily adaptable. 

\subsection{Hierarchical Principal Manifold Estimation}

With the structure of the additive spline model established, we now incorporate it into the larger manifold learning procedure. This procedure is based on the Principal Manifold Estimation (PME) algorithm introduced in \cite{mengPrincipalManifoldEstimation2021}, which iterates between estimating a function to embed observations in $D$-dimensional space and reparameterizing those observations in the $d$-dimensional space to estimate the underlying manifold.

\section{Application to Simulated and MNIST Data}

\subsection{Application to Simulated Data}

\subsection{Application to MNIST Data}

\section{Application to ADNI Data}

\section{Discussion}

\newpage

\nocite{*}
%\bibliographystyle{plain}
%\bibliography{references}
\printbibliography

\end{document}
